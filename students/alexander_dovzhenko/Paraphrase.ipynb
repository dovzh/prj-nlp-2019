{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paraphrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_tags(string):\n",
    "    return [tuple(i.split(\"/\")) for i in string.split()]\n",
    "\n",
    "def readTrainData(filename):\n",
    "    data = []\n",
    "    for line in open(filename):\n",
    "        line = line.strip()\n",
    "        #read in training or dev data with labels\n",
    "        if len(line.split('\\t')) == 7:\n",
    "            (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = \\\n",
    "            line.split('\\t')\n",
    "        else:\n",
    "            continue\n",
    "        # ignoring the training data that has middle label \n",
    "        nYes = eval(judge)[0]            \n",
    "        if nYes >= 3:\n",
    "            amt_label = True\n",
    "            data.append((split_tags(origsenttag), split_tags(candsenttag), amt_label))\n",
    "        elif nYes <= 1:\n",
    "            amt_label = False\n",
    "            data.append((split_tags(origsenttag), split_tags(candsenttag), amt_label))\n",
    "    return data\n",
    "\n",
    "def readTestData(filename):\n",
    "    data = []\n",
    "    for line in open(filename):\n",
    "        line = line.strip()\n",
    "        #read in training or dev data with labels\n",
    "        if len(line.split('\\t')) == 7:\n",
    "            (trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = \\\n",
    "            line.split('\\t')\n",
    "        else:\n",
    "            continue\n",
    "        # ignoring the training data that has middle label \n",
    "        nYes = int(judge[0])\n",
    "        if nYes >= 4:\n",
    "            expert_label = True\n",
    "        elif nYes <= 2:\n",
    "            expert_label = False\n",
    "        else:\n",
    "            expert_label = None\n",
    "        data.append((split_tags(origsenttag), split_tags(candsenttag), expert_label))\n",
    "    return data\n",
    "\n",
    "train_data = readTrainData(\"SemEval-PIT2015-py3/data/train.data\")\n",
    "dev_data = readTrainData(\"SemEval-PIT2015-py3/data/dev.data\")\n",
    "test_data = readTestData(\"SemEval-PIT2015-py3/data/test.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyemd import emd\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"numberbatch-en.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(words):\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    words = [w for w in words if len(w) > 3]\n",
    "    words = [lem.lemmatize(w) for w in words]\n",
    "    return words\n",
    "\n",
    "def sample_x(sample, sub=False):\n",
    "    get_words = lambda ann_tw: [ann_w[0] for ann_w in ann_tw]\n",
    "    to_vec = lambda w : word2vec_model[w] if w in word2vec_model else np.zeros(300)\n",
    "    \n",
    "    def tw_vec(vecs):\n",
    "        if not vecs:\n",
    "            return np.zeros(300)\n",
    "        return np.sum(np.array(vecs), axis=0) / len(vecs)\n",
    "    \n",
    "    text0 = preprocess_text(get_words(sample[0]))\n",
    "    text1 = preprocess_text(get_words(sample[1]))\n",
    "\n",
    "    vec0 = tw_vec([to_vec(w) for w in text0])\n",
    "    vec1 = tw_vec([to_vec(w) for w in text1])\n",
    "    #dist = model.wmdistance(get_words(sample[0]), get_words(sample[1]))\n",
    "    #dist = min(dist, 100)\n",
    "\n",
    "    if sub:\n",
    "        return vec0 - vec1\n",
    "    else:\n",
    "        return np.concatenate((vec0, vec1), axis=None)\n",
    "\n",
    "    #return np.concatenate((vec0, vec0), axis=None)\n",
    "    #return {'vecs0': vecs0, 'vecs1': vecs1}\n",
    "\n",
    "def sample_y(sample):\n",
    "    return sample[2]\n",
    "\n",
    "X_train = [sample_x(sample) for sample in train_data]\n",
    "Y_train = [sample_y(sample) for sample in train_data]\n",
    "\n",
    "X_dev = [sample_x(sample) for sample in dev_data]\n",
    "Y_dev = [sample_y(sample) for sample in dev_data]\n",
    "\n",
    "X_test = [sample_x(sample) for sample in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.65      0.76      3862\n",
      "        True       0.07      0.36      0.12       280\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      4142\n",
      "   macro avg       0.50      0.51      0.44      4142\n",
      "weighted avg       0.87      0.63      0.72      4142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lrc = LogisticRegression(random_state=42, solver=\"sag\", max_iter=1000)\n",
    "lrc.fit(X_train, Y_train)\n",
    "print(classification_report(lrc.predict(X_dev), Y_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(train_vecs):\n",
    "    results = np.zeros((len(train_vecs), len(train_vecs[0])))\n",
    "    for i, train_vec in enumerate(train_vecs):\n",
    "        results[i] = train_vec\n",
    "    return results\n",
    "\n",
    "X_train = vectorize_sequences([sample_x(sample, True) for sample in train_data])\n",
    "X_test = vectorize_sequences([sample_x(sample, True) for sample in test_data])\n",
    "Y_train = np.asarray(Y_train).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11530/11530 [==============================] - 1s 71us/sample - loss: 0.6478 - acc: 0.6487\n",
      "Epoch 2/5\n",
      "11530/11530 [==============================] - 1s 52us/sample - loss: 0.6265 - acc: 0.6558\n",
      "Epoch 3/5\n",
      "11530/11530 [==============================] - 1s 54us/sample - loss: 0.6064 - acc: 0.6660\n",
      "Epoch 4/5\n",
      "11530/11530 [==============================] - 1s 52us/sample - loss: 0.5898 - acc: 0.6689\n",
      "Epoch 5/5\n",
      "11530/11530 [==============================] - 1s 50us/sample - loss: 0.5768 - acc: 0.6753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History object at 0x10862d160>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(10, activation='relu', input_shape=(300,)),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutputPredictions(Y_pred, outfile):\n",
    "    # output the results into a file\n",
    "    outf = open(outfile,'w') \n",
    "\n",
    "    for y in Y_pred:\n",
    "        if y >= 0.5:\n",
    "            outf.write(\"true\\t\" + \"{0:.4f}\".format(y) + \"\\n\")\n",
    "        else:\n",
    "            outf.write(\"false\\t\" + \"{0:.4f}\".format(y) + \"\\n\")\n",
    "\n",
    "    outf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sasha/Documents/prj-nlp-2019_/practice_11\n"
     ]
    }
   ],
   "source": [
    "%cd -\n",
    "\n",
    "#Y_pred = [1] * len(test_data)\n",
    "#Y_pred = lrc.predict(X_test)\n",
    "#OutputPredictions(Y_pred, 'SemEval-PIT2015-py3/systemoutputs/PIT2015_SASHA_01_LG.output')\n",
    "\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "Y_pred = [int(y[0]>y[1]) for y in Y_pred]\n",
    "OutputPredictions(Y_pred, 'SemEval-PIT2015-py3/systemoutputs/PIT2015_SASHA_01_FNN.output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sasha/Documents/prj-nlp-2019_/practice_11/SemEval-PIT2015-py3/scripts\n",
      "838\tBASELINE\t02_LG\t\tF: 0.589\tPrec: 0.679\tRec: 0.520\t\tP-corr: 0.511\tF1: 0.601\tPrec: 0.674\tRec: 0.543\n",
      "838\tSASHA\t01_LG\t\tF: 0.163\tPrec: 0.515\tRec: 0.097\t\tP-corr: 0.131\tF1: 0.389\tPrec: 0.258\tRec: 0.783\n",
      "838\tSASHA\t01_FNN\t\tF: 0.344\tPrec: 0.209\tRec: 0.983\t\tP-corr: -0.027\tF1: 0.375\tPrec: 0.251\tRec: 0.743\n"
     ]
    }
   ],
   "source": [
    "%cd SemEval-PIT2015-py3/scripts\n",
    "!python3 pit2015_eval_single.py ../data/test.label ../systemoutputs/PIT2015_BASELINE_02_LG.output\n",
    "!python3 pit2015_eval_single.py ../data/test.label ../systemoutputs/PIT2015_SASHA_01_LG.output\n",
    "!python3 pit2015_eval_single.py ../data/test.label ../systemoutputs/PIT2015_SASHA_01_FNN.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
